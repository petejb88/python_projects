{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "import os\n",
    "import inspect\n",
    "import random\n",
    "import shutil\n",
    "import json\n",
    "from collections import OrderedDict\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_filename = inspect.getframeinfo(inspect.currentframe()).filename\n",
    "working_path = os.path.dirname(os.path.abspath(this_filename))\n",
    "data_path = working_path+\"/../input/covid-chest-xray\"\n",
    "images_path = data_path+\"/images\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataSet(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self,csv_path,images_path,these_indices=None,transforms=None):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            - csv_path: path to csv file with metadata\n",
    "            - images_path: path to images\n",
    "            - these_idx: list of indices to keep in this dataset\n",
    "            - transform: transforms to be applied\n",
    "        \"\"\"\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.images_path = images_path\n",
    "        self.these_indices = these_indices\n",
    "        self.transforms = transforms        \n",
    "        \n",
    "        # remove rows that don't include the appropriate filenames:\n",
    "        if self.these_indices:\n",
    "            self.df.drop([idx for idx in self.df.index if idx not in these_indices], inplace=True)\n",
    "        \n",
    "        # Data Processing:\n",
    "        # if view not PA, drop the row\n",
    "        self.df.drop(self.df[self.df.view != 'PA'].index, inplace=True)\n",
    "        # if image DNE, drop the row\n",
    "        self.df.drop([idx for idx in self.df.index if self.df.filename[idx] not in os.listdir(images_path)], inplace=True)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        'Generates one sample of data'\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        image_path = images_path+\"/\"+self.df['filename'].iloc[idx]\n",
    "        image = Image.open(image_path)\n",
    "        if self.transforms:\n",
    "            image = self.transforms(image)\n",
    "        \n",
    "        if self.df['finding'].iloc[idx] != 'COVID-19':\n",
    "            finding = 0\n",
    "        else:\n",
    "            finding = 1\n",
    "            \n",
    "        return image, finding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create train and valid datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,  12,\n",
       "            ...\n",
       "            348, 349, 350, 351, 352, 353, 356, 357, 358, 364],\n",
       "           dtype='int64', length=195)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_dataset = ImageDataSet(data_path+\"/metadata.csv\",images_path)\n",
    "all_dataset.df.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomly split all_dataset into training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([transforms.RandomRotation(90),\n",
    "                                       transforms.RandomResizedCrop(224),\n",
    "                                       transforms.RandomHorizontalFlip(),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize(\n",
    "                                           [0.485, 0.456, 0.406], \n",
    "                                           [0.229, 0.224, 0.225])])\n",
    "valid_transforms = transforms.Compose([transforms.CenterCrop(224),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize(\n",
    "                                           [0.485, 0.456, 0.406], \n",
    "                                           [0.229, 0.224, 0.225])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_train_valid_loaders(csv_path,images_path,train_transform,valid_transforms,split_ratio,batchsize):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        - csv_path: path to csv file with metadata\n",
    "        - images_path: path to images\n",
    "        - train/valid transform: transforms to be applied to training, validation data\n",
    "        - split_ratio: float between 0,1 indicating percentage of data to split into validation\n",
    "        - batchsize = batch size\n",
    "    \"\"\"         \n",
    "    \n",
    "    all_dataset = ImageDataSet(csv_path,images_path)\n",
    "    num_images = len(all_dataset)\n",
    "    # all_dataset already does some data processing\n",
    "    valid_indices = random.sample(list(all_dataset.df.index),int(split_ratio*num_images))\n",
    "    train_indices = [idx for idx in all_dataset.df.index if idx not in valid_indices]\n",
    "    valid_dataset = ImageDataSet(csv_path,images_path,valid_indices,valid_transforms)\n",
    "    train_dataset = ImageDataSet(csv_path,images_path,train_indices,train_transforms)\n",
    "    \n",
    "    trainloader = torch.utils.data.DataLoader(train_dataset, batch_size = batchsize, shuffle = True)\n",
    "    validloader = torch.utils.data.DataLoader(valid_dataset, batch_size = batchsize)\n",
    "    \n",
    "    print(\"Data loaded!\")\n",
    "    return trainloader, validloader\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, criterion, optimizer = build_model(\"alexnet\",0.01,[1024,1024],train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, optimizer, train_losses, valid_losses, accuracy_data = train_model(model,criterion,optimizer,trainloader,validloader,6,5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_stats(train_losses, valid_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(valid_losses) == len(train_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2\n",
    "plt.plot(range(n,len(train_losses)), train_losses[n:], label=\"train losses\")\n",
    "plt.plot(range(n,len(train_losses)), valid_losses[n:], label=\"valid losses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_checkpoint(model,optimizer,\"alexnet\",0.01,6,working_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = 0\n",
    "column=\"is_covid\"\n",
    "cat_to_name, name_to_cat = make_cat_dicts(images_path,data[column].unique())\n",
    "images, images_in_data, data_not_in_images = list_images_in_data(images_path)\n",
    "predicted_covid = 0\n",
    "actual_covid = 0\n",
    "for image in images_in_data:\n",
    "    image_data = data[data['filename']==image]\n",
    "    image_index = image_data.index[0]\n",
    "    image_column = image_data[column][image_index]\n",
    "    image_cat = name_to_cat[image_column]\n",
    "\n",
    "    top_prob, top_class = predict(images_path+\"/\"+image,model,1,False)       \n",
    "    accuracy += (top_class[0] == image_cat) \n",
    "    if top_class[0] == \"1\":\n",
    "        predicted_covid += 1\n",
    "    if image_cat == \"1\":\n",
    "        actual_covid += 1\n",
    "print(\"Accuracy: {}\".format(accuracy / len(images_in_data)))\n",
    "print(\"Predicted Covid: {}, Actual Covid: {}\".format(predicted_covid,actual_covid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_checkpoint(working_path+\"/../workingcheckpoint.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = images_in_data[0]\n",
    "image_data = data[data['filename']==image]\n",
    "image_index = image_data.index[0]\n",
    "image_column = image_data[column][image_index]\n",
    "image_cat = name_to_cat[image_column]\n",
    "\n",
    "top_prob, top_class = predict(images_path+\"/\"+image,model,1,False)\n",
    "print(top_prob, top_class)\n",
    "print(image_cat)\n",
    "print(top_class[0] == image_cat)\n",
    "print_predictions(top_prob, top_class,images_path+\"/../cat_to_name.json\")\n",
    "data[data['filename']==image]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "275/len(images_in_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = 0\n",
    "for i in range(1000):\n",
    "    top_prob, top_cat = predict(images_path+\"/\"+image,model,1,False)\n",
    "    precision += int(top_cat[0])\n",
    "print(precision/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
