{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different Loaders and Samplers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Type 1: ImageFolder with train/valid subfolders, no weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_loader(data_dir,batchsize,train_transforms,valid_transforms):\n",
    "\n",
    "    train_dir = data_dir+\"/train/\"\n",
    "    valid_dir = data_dir+\"/valid/\"\n",
    "    \n",
    "    train_data = datasets.ImageFolder(train_dir, transform = train_transforms)\n",
    "    valid_data = datasets.ImageFolder(valid_dir, transform = valid_transforms)\n",
    "    \n",
    "    trainloader = torch.utils.data.DataLoader(train_data, batch_size = batchsize, shuffle = True)\n",
    "    validloader = torch.utils.data.DataLoader(valid_data, batch_size = batchsize)\n",
    "    \n",
    "    print(\"Data loaded\")\n",
    "    return(train_data, valid_data, trainloader, validloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Type 2: ImageFolder with no train/valid split, no weights\n",
    "Make train/valid_datasets everything (with appropriate transforms), and then split in the sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_loader(data_dir, train_transforms, valid_transforms, valid_ratio, batchsize):\n",
    "    \n",
    "    train_dataset = datasets.ImageFolder(data_dir, transform = train_transforms)\n",
    "    valid_dataset = datasets.ImageFolder(data_dir, transform = valid_transforms)\n",
    "    \n",
    "    valid_indices = random.sample(list(all_dataset.df.index),int(split_ratio*num_images))\n",
    "    train_indices = [idx for idx in all_dataset.df.index if idx not in valid_indices]\n",
    "\n",
    "    train_sampler = torch.utils.data.sampler.RandomSubsetSampler(train_indices)\n",
    "    valid_sampler = torch.utils.data.sampler.RandomSubsetSampler(valid_indices)\n",
    "\n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size = batchsize, sampler = train_sampler)\n",
    "    validloader = torch.utils.data.DataLoader(\n",
    "        valid_dataset, batch_size = batchsize, sampler = valid_sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Type 3: Folder of images\n",
    "New dataset class: builds dataset on indices, removes images not in dataset, can filter other features\n",
    "build_train_valid_loaders(): splits dataset into train/valid_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-afa957895e03>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mImageDataSet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcsv_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimages_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mthese_indices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \"\"\"\n\u001b[1;32m      5\u001b[0m         \u001b[0mInput\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "class ImageDataSet(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self,csv_path,images_path,these_indices=None,transforms=None):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            csv_path: path to csv file with metadata\n",
    "            images_path: path to images\n",
    "            these_indices: list of indices to keep in this dataset\n",
    "            transform: transforms to be applied\n",
    "        \"\"\"\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.df.set_index(\"filename\", inplace = True)\n",
    "        self.images_path = images_path\n",
    "        self.these_indices = these_indices\n",
    "        self.transforms = transforms\n",
    "        \n",
    "        # remove rows that don't include the appropriate filenames:\n",
    "        if self.these_indices:\n",
    "            self.df.drop([idx for idx in self.df.index if idx not in these_indices], inplace=True)\n",
    "            \n",
    "        # Data Processing:\n",
    "        # if view not PA, drop the row\n",
    "        self.df.drop(self.df[self.df.view != 'PA'].index, inplace=True)\n",
    "        # if image DNE, drop the row\n",
    "        self.df.drop([idx for idx  in self.df.index if self.df.filename[idx] not in os.listdir(images_path)], inplace=True)\n",
    "        \n",
    "        def __len__(self):\n",
    "            return len(self.df)\n",
    "        \n",
    "        def __getitem__(self, idx):\n",
    "            'Generates one sample of data'\n",
    "            if torch.is_tensor(idx):\n",
    "                idx = idx.tolist()\n",
    "            \n",
    "            image_path = images_path+\"/\"+self.df['filename'].iloc[idx]\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "            if self.transforms:\n",
    "                image = self.transforms(image)\n",
    "                \n",
    "            if self.df['finding'].iloc[idx] != 'COVID-19':\n",
    "                finding = 0\n",
    "            else:\n",
    "                finding = 1\n",
    "                    \n",
    "            return image, finding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_train_valid_loaders(csv_path,images_path,train_transform,valid_transforms,split_ratio,batchsize):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        - csv_path: path to csv file with metadata\n",
    "        - images_path: path to images\n",
    "        - train/valid transform: transforms to be applied to training, validation data\n",
    "        - split_ratio: float between 0,1 indicating percentage of data to split into validation\n",
    "        - batchsize = batch size\n",
    "    \"\"\"\n",
    "\n",
    "    all_dataset = ImageDataSet(csv_path,images_path)\n",
    "    num_images = len(all_dataset)\n",
    "    # all_dataset already does some data processing: removes images not in data and images not PA\n",
    "\n",
    "    \n",
    "    # build train, valid datasets\n",
    "    valid_indices = random.sample(list(all_dataset.df.index),int(split_ratio*num_images))\n",
    "    train_indices = [idx for idx in all_dataset.df.index if idx not in valid_indices]\n",
    "    valid_dataset = ImageDataSet(csv_path,images_path,valid_indices,valid_transforms)\n",
    "    train_dataset = ImageDataSet(csv_path,images_path,train_indices,train_transforms)\n",
    "\n",
    "    # build weights\n",
    "    train_covid = len(train_dataset.df[train_dataset.df['finding'] == 'COVID-19'])\n",
    "    train_class_weights = [train_covid / (len(train_dataset) - train_covid), (len(train_dataset) - train_covid) / train_covid]\n",
    "    train_weights = [train_class_weights[finding] for image,finding in train_dataset]\n",
    "    \n",
    "    valid_covid = len(valid_dataset.df[valid_dataset.df['finding'] == 'COVID-19'])\n",
    "    valid_class_weights = [valid_covid / (len(valid_dataset) - valid_covid), (len(valid_dataset) - valid_covid) / valid_covid]\n",
    "    valid_weights = [valid_class_weights[finding] for image,finding in valid_dataset]\n",
    "\n",
    "    # build samplers and loaders\n",
    "    train_sampler = torch.utils.data.sampler.WeightedRandomSampler(train_weights, len(train_dataset))\n",
    "    valid_sampler = torch.utils.data.sampler.WeightedRandomSampler(valid_weights, len(valid_dataset))\n",
    "    \n",
    "    trainloader = torch.utils.data.DataLoader(train_dataset, batch_size = batchsize, sampler = train_sampler)\n",
    "    validloader = torch.utils.data.DataLoader(valid_dataset, batch_size = batchsize, sampler = valid_sampler)\n",
    "    \n",
    "    print(\"Data loaded!\")\n",
    "    return trainloader, validloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying Weights\n",
    "Can apply weights to the above in two different ways:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. if there are only two classes, and the class is given by a value of a dataframe\n",
    "(manipulating dataframes is in general much faster than iterating through the whole dataset):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-2a34d1bc2aab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_covid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'finding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'COVID-19'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtrain_class_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtrain_covid\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtrain_covid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtrain_covid\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtrain_covid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtrain_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtrain_class_weights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfinding\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfinding\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mvalid_covid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalid_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'finding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'COVID-19'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "train_covid = len(train_dataset.df[train_dataset.df['finding'] == 'COVID-19'])\n",
    "train_class_weights = [train_covid / (len(train_dataset) - train_covid), (len(train_dataset) - train_covid) / train_covid]\n",
    "train_weights = [train_class_weights[finding] for image,finding in train_dataset]\n",
    "    \n",
    "valid_covid = len(valid_dataset.df[valid_dataset.df['finding'] == 'COVID-19'])\n",
    "valid_class_weights = [valid_covid / (len(valid_dataset) - valid_covid), (len(valid_dataset) - valid_covid) / valid_covid]\n",
    "valid_weights = [valid_class_weights[finding] for image,finding in valid_dataset]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Iterate through the samples of the dataset# build weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Counter' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-44eb04ad1052>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mclass_counts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mclass_counts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mclass_counts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'valid'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalid_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrain_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mclass_counts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Counter' is not defined"
     ]
    }
   ],
   "source": [
    "class_counts = {}\n",
    "class_counts['train'] = dict(Counter(sample[1] for sample in train_dataset))\n",
    "class_counts['valid'] = dict(Counter(sample[1] for sample in valid_dataset))\n",
    "\n",
    "train_weights = [class_counts['train'][sample[1]] for sample in train_dataset]\n",
    "valid_weights = [class_counts['valid'][sample[1]] for sample in valid_dataset]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In both cases, building the sampler and loaders remains the same:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-9088a8f84ac4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m train_sampler = torch.utils.data.sampler.WeightedRandomSampler(\n\u001b[0m\u001b[1;32m      2\u001b[0m     train_weights, len(train_dataset))\n\u001b[1;32m      3\u001b[0m valid_sampler = torch.utils.data.sampler.WeightedRandomSampler(\n\u001b[1;32m      4\u001b[0m     valid_weights, len(valid_dataset))\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "train_sampler = torch.utils.data.sampler.WeightedRandomSampler(\n",
    "    train_weights, len(train_dataset))\n",
    "valid_sampler = torch.utils.data.sampler.WeightedRandomSampler(\n",
    "    valid_weights, len(valid_dataset))\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size = batchsize, sampler = train_sampler)\n",
    "validloader = torch.utils.data.DataLoader(\n",
    "    valid_dataset, batch_size = batchsize, sampler = valid_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
